"timestamp","name","organization","email","comment"
"2020-07-07T12:17:05.000","Steven Surbaugh","OHA","steven.t.surbaugh@state.or.us","Governance and Effective Management
Govern:
1.        There is belief in the executive level that there is neither time nor resources to staff this level of effort; let alone sustain it. How do we overcome that, how do we make sure there is time and resources made available to mature data management practices; because the maturing of data management practices is what we are talking about.
2.        “Build data governance Centers of Excellence”; what does this mean; how is it defined; who is in charge of it?
3.        “Inventory and identify high-value and mission-critical data assets within the State’s possession”. So many questions on this one. 1. Who is performing the inventory? 2. How is the inventory maintained over time? 3. Who is classifying high-value/mission-critical data assets? 4. What is the criteria for classifying a data asset as “high-Value” or “mission-critical”?
4.        Is there an universal assessment tool available for use for the data governance maturity assessment? What is the maturity scale and the target goals for each maturity level? Who performs this assessment; is the assessment performed internally; contracted out to remove any bias; or cross agency “peer-to-peer” auditing?
Leverage:
5.        “Recognize the value of data beyond the initial purpose for which it was collected and work to identify new and innovative ways to use data…” This requires a complete inventory of data assets and the in case of DHS and OHA, this include over 400 applications, over 2000 databases used by both agencies and supported by OIS’s technical staff, of less than 300 people. And while we have technical capability to begin this inventory function, we have no efforts to apply or properly support this capability to our systems. Furthermore, this level of work and thinking takes time and a dedicated team not focused on firefighting the hundreds of day to day issues that arise; this requires data strategists; this requires a business data strategy; this requires out-of-the-box thinking; this requires a high level of data management maturity to begin this level of work.
6.        “Contextualize our data through the creation and dissemination of data documentation; e.g., data dictionaries, schemata, and standards” Some divisions in agencies are top tier, far more mature than other agencies, when it comes to this; like ODOTs ITS. Where other agencies, like DHS/OHA have been spending 30 years building and buying silos of data without any forward thinking or standards applied. What is the strategy for remediation of systems that do not comply with the developed standards? What is the strategy for ensuring any future “transfer” or “COTS” acquisitions comply with these standards?
7.        “Implement, adopt, and create shared data standards whenever possible to facilitate re-use and data sharing”… At what level? Is this within agencies and across their program areas? Is this data standard sharing between agencies? What is the context of this?

Protect:
8.        “Build policies and guidelines to safeguard data quality, integrity, and authenticity”; this is a Level 3 to Level 4 data maturity practice. For our supported agencies, this level a maturity is five years away. This involving “knowing” our data assets, and knowing where these assets reside. For example, today we have 60+ individual applications that collect and manage “client” data independently of each other with no standards applied and in many cases, very little integrity and quality measures applied.
9.        Privacy policies are in place, but can be, and are often disregarded, especially when costs are considered, with the business “accepting the risk”. This often, in the past, included PII, HIPAA and other sensitive data having their classification reduced to avoid what where considered costly security measures. This policy loop hole is also what allowed the 2019 DHS data breach to happen… because policy was circumvented in the name of risk acceptance to save money, PII data was sent via being email, so when our email system was breached via a phishing scam, client’s sensitive data was exposed, unnecessarily. What is the strategy to get a statewide Privacy First initiative in place?
10.        Where are these privacy guidelines going to reside; who will own them; who will audit and enforce them? Will these guidelines offer specific use case examples; e.g. PII = Level 3, HIPAA = Level 4, Child Data, under the age of 13 = Level 4, etc? Will there be a risk acceptance loophole negating any potential security gains?  
Share:
11.        Can we also address the political/funding barriers that block many data sharing opportunities? Will this be done through a centralized source, or a statewide Master Data Management capability; e.g. Citizen Master ID, that include social security numbers, Oregon ID/Driver’s Licence numbers, etc. for person identity matching? Will it also include a central address verification system available to all state agencies; like that many agencies and programs already subscribe to and pay for independently to verify addresses, many time paying to verify the same address multiple times; Melissa Address Verification for example.
12.        Who will be heading this effort and how does a person, like myself get involved?
13.        Would really like to see this become reality.
Ethical Use
Plan:
14.        Really this comment is 14, 5, 16 and 17. We need to incorporate a complete data lifecycle planning not only into projects, but as a part of Data Governance.  
Engage:
18.        “Create engagement opportunities for, and give voice to, underrepresented communities regarding the State’s data practices through opportunities for feedback and by supporting data literacy and data sharing” This starts at the capitol building; with the legislature, the governor and the senate. My case in point, the REAL+D bill was passed into law mandating the collection of Race, Ethnicity and Language plus Disability data. The problem is, two-fold; the definitions of minimally required Race categories intermingled race and ethnicity. Whereas race is mostly defined and determined by physical characteristics, ethnicity is more about a person's culture, language, family and place of origin. ... Examples of ethnicity include being Indian, Jewish or Asian, regardless of race. The second issue is the limited number of measured disabilities; as Blindness is listed a disability, but amputee is not. This would imply that certain disabilities are recognized by the state, while others are not.

19.        “Apply an equity lens to how we collect and use data entrusted to the state”. What does this mean? IS this a reference to removing implicit and explicit bias from the collection and use of data?
20.        Yes… and developed a strategy to resolve these limitations.
21.        Yes… Please. 

Show:
22.        Really everything under the show header is a yes."
"2020-07-08T16:15:25.000","Antonio R. Vargas","DCBS","antonio.r.vargas@oregon.gov","This is a reasonable outline of good principles and practices. In its current form it's too vague to be useful. // Maybe it could point to some detailed case studies explaining how some of the items have actually been implemented somewhere? // I'll also second Steven Surbaugh's comment -- there's some really good stuff in there."
"2020-07-09T06:53:29.000","Bill Bahl","Oregon State Hospital","william.bahl@state.or.us","I’d recommend two parallel paths:  do what you have in your plan outline and teach managers how to ask the appropriate questions.  When someone approaches a manager and suggests something has “increased,”  the manager should ask questions such as, “What data indicates this?”, “What hypothesis test did you use?”, “What was the sample size and during what period were the samples taken?”, “What are the limitations of the data set?”  If managers indicate up front that they are unwilling to waste resources on anecdotal evidence, our system will move toward evidence-based decision making."
"2020-07-09T10:40:53.000","James S. Kagan","Institute for Natural Resources, Oregon State University","jimmy.kagan@oregonstate.edu","Briefly, can you tell me what “Build data governance Centers of Excellence by policy area and across shared business functions” means. It sounds either promising or a potential new entity to complete with existing cooperative programs.

I like the idea of sharing info for a plan like this on the web as you have, but without links to actual information showing what these statements are supposed to me, they can be perceived as platitudes."
"2020-08-06T11:45:10.000","Jerry Power","I3 Systems","Jerry.power@i3-iot.net","1) Responsible Data Sharing is an appropriate principal.  A requirement that comes with this principal is the need to periodically audit data sharing partners to ensure the partners are not violating Oregon's principals and practices.  Many privacy and security breaches come from 3rd parties who do not adequately protect the state's data. 
2) Organization assessments and partner data audits should be conducted on a periodic and regular bases to ensure a culture of continuous improvement (continuous improvement of the data practices).   Responsible data sharing requires transparency of the data collected, the use cases that are driving the collection process, and the level of sharing that has been authorized between departments and with external partners.  A part of the audit is to ensure the data is not used by any party outside the defined/permitted parameters.     
4) There is no mention of maintainability.  Also does not mentions resilience/reliability.   Data and information networks can be expensive to maintain and support and these costs can be expensive.  Suggest saying something about the managing the life cost of the architecture."
"2020-08-16T16:49:07.000","Kristin Wolff","Social Policy Research/thinkers+doers","kwolff@thinkers-and-doers.com","This is a super list of principles. It would benefit greatly from two things: 1) A mental model or picture of what Oregon's approach to data will be. The introduction speaks to consistency but not to value. Isn't the whole idea to use data to improve collaborative problem-solving (or something similar)? That's an applied approach to governance. There are many models for this: data collaboratives, data trusts, data partnerships, etc. but it's not clear what we're talking about here - or whether it differs based on the area of focus (e.g. education and health data are governed by different law, for example). It's also not clear whether the role of the center (e.g. state agency) is about compliance (e.g. making sure agencies adhere) or capacity building (taking ownership of learning, training, and partnership development). 2) A greater focus on data use and users, including those external to state governance, and more specificity around the who and how of implementation. Many of the practices are written as if they are internal to agencies and processes, but data use is about cultivating a community of data providers, partners, analysts, users, etc. who can learn together (across nonprofits, service deliverers, private sector partners, etc.). There's a whole level of visioning and convening that's necessary for working effectively with public data that's not really addressed. It might be assumed to exist within the Centers of Excellence -- but I'm not clear about that either. Is the ""excellence"" intended to be about data sharing and governance (adhering to principles and practices or data applicition and use (e.g. communities of practices within specific data domains)? Finally, COVID will force a level of collaboration we're likely unaccustomed to, if for no other reason, resource constraints. We have to learn fast. I'm so pleased Oregon is doing this. I also do not underestimate the gigantic change (cultural, technological, business-process-wise, etc.) we're talking about here. So much excellent work in this area right now via the Beeck Center at Georgetown, Schmidt Futures, Johns Hopkins Center for Civic Impact, AISP, and a whole bunch of domain-specific partners are engaged in this work from a state perspective and developing loads of resources. We should also think about how we participate in any number of learning communities focused on data and data-informed policy right now -- especially those that bridge cities and states."
"2020-08-17T13:21:26.000","Ralph Fry","ODOT","ralph.fry@odot.state.or.us","It would be highly beneficial to include under ""Governance and Effective Management"", an integrated approach to the related policy areas of:  Information Asset Classification, and Records Management.  An integration PRINCIPLE could be something like, ""Integrate overlapping areas of policy were the management of data intersects with information asset classification, records management to gain efficiency.   In the GOVERNANCE and EFFECTIVE MANAGEMENT areas, under GOVERN , #2 wording could be added, to specify strategically that ""by policy area"" is to include but is not limited to, information asset classification and record management.   Under PROTECT #8, Integration would fit well with ""building policy and guidelines"" regarding the policy for information asset classification and records.   Please add the important distinction that the enterprise level  (DAS / EIS DATA, CSS) would follow the data strategy and produce integrated consistent statewide policy covering data, information asset classification and records management.  So it makes little sense to have each agency try to integrate divergent statewide policies - take extra time and do it right at the enterprise level.  To note current policy in these areas is over 10 years old.   A good set will likely have a long shelf life, so worth doing effectively!"
"2020-08-21T08:14:32.000","Multnomah County, Oregon","Multnomah County, Oregon",,"Multnomah County supports this excellent work and appreciates that this strategy is based on the Federal Data Strategy. 

Other comments:

Oregon’s Identified Data Practices

Governance and Effective Management
Govern
2. What are the Centers of Excellence?  Is it the same as a community of excellence in #24?
3. Recommend defining “value” and “high”. What are ""high-value""... data assets?  High-value to who?  Is this data that is high-value to the individual that it pertains to (sensitive or if lost could result in personal harm)? Or high-value in monetary value to the State?  

Leverage
5. ""Recognize the value of data beyond the initial purpose......""  How will the State ensure that individuals know these downstream uses of identifiable data.  Can an individual opt out?  

Protect
8. What about availability or accuracy?
9. Even if policies/procedures are in place, are they monitored to ensure that they are being followed?  What is critical data?

Share 
General comment.  External partners are referenced in the general description but not in #s 11, 12, or 13.  Will external partners be audited or monitored?  Contractually agree to these principles?
11. What are ""cultural barriers"" to overcome?  Are these State cultural barriers (e.g. departments or agencies within the State) or cultural barriers in the community?  What are the barriers?  Laws?  Policies/procedures? How is this different from #23?

Ethical Use
Plan
15. Should also include use and disclosure

Engage
21. How we use data to make decisions should also include how the State asks others to use. Should also include ""disclose.""
23. How is this different from #11?

Show 
25. Do “automated data pipelines” include artificial intelligence?  If so, how will the State be sure decisions are made without bias? 

Data Informed Culture
Learn
28. Should also include disclosure"
"2020-08-21T14:11:23.000","Brian Tong","OSCIO/EIS/DCS","brian.tong@oregon.gov","As I read this, it occurs to me that I really do not have a sense for how wide the gap is from where we are today to where we are planning to go. It might be helpful to spend a few slides on examples of the ""The Problem"""
"2020-08-24T12:13:15.000","Leela Yellesetty","Department of Environmental Quality","leela.yellesetty@deq.state.or.us","DEQ Comments on the Draft State Data Strategy

The following is a summary of comments from DEQ’s Information Governance Council and R Users Group (data analysts) regarding the draft Oregon Data Strategy. Thanks for your consideration.

General Comments:

•	This is a good first step and welcome sign that the state is trying to develop a uniform data strategy across agencies.
•	High level principles are good, but the lower level items are lacking in details.
•	DEQ wasn’t one of the agencies involved in the committee to draft the strategy—this is very concerning given how much data we manage and its unique nature.
•	It would be useful to spell out a hierarchy between the Chief Data Officer and staff in each agency. Who in each agency is going to be responsible for making sure the Data Strategy is being upheld? Will there be any kind of audit? 
•	How would it be implemented on the agency level? At DEQ it is difficult to manage across programs within the agency. Would there be resources available to help agencies implement? Training? It may be useful to assemble a group of agency data officers to work together on implementation.
•	What is the scale of the proposed data literacy program? Will it be offered statewide or only to  specific agencies/programs?
•	What detail can we expect in the Action Plan? Will we be able to review and provide comments on the Action Plan as well?
•	DEQ along with other agencies are embarking on major rollouts of new systems (e.g. EDMS). Will this strategy have implications for the architecture of these systems and/or new procurements? Especially as it relates to data sharing between agencies.

Defining Data:

•	Data is not clearly defined in the strategy. We would like a definition.
•	What is the difference between data and information? Does data include other kinds of records/ unstructured data (emails, reports, etc.)? 
•	Is environmental data different from other data in the State's provenance?
•	Data is kind of useless without some context (metadata, READMEs, etc), and processing information (methods explanation and/or code).  Model outputs (like Total Maximum Daily Load) could also be considered data but different from monitoring data. In order to reproduce the results would need the inputs and the code. If we want transparency and reproducibility the code must be part of the data.
•	Is there a need to distinguish between raw data and derived data? DEQ collects monitoring data by the minute and averages at various intervals. Important to keep both raw and derived and document how the raw was converted to derived. Sometimes conversion methods change over time. Also different rules (such as EPA requirements) may determine how we derive the data.
•	What metadata needs to be associated with raw & derived data? Should clean-up and algorithms used to derive data be considered part of the metadata?
•	Will there be guidelines for sharing data between agencies? DEQ’s Water Quality program can share its guidelines with others, including YouTube videos. Would be good to have consistency on minimum requirements/ quality control.
•	Are there standards for units and projections across the state? The state Geospacial Office has standards but not sure if everyone using them. Perhaps the strategy could include an appendix with links to these sorts of standards. Will DAS work with the Geospacial Office and other groups on standards for different types of data?
•	We see no reference to national or international standards (e.g. ISO, ANSI) in the strategy. Were these consulted? We shouldn’t be reinventing the wheel. 
 
Public good and Ethical use:

•	What does it mean for us to steward data as a ""public good""? What implications does this have for data collection, data analysis, and data communication? Do we have best practices to guide us? Will the State provide us with guidance?
•	Referring to data as an “asset” makes it seem like it should be controlled. Should state more clearly that it belongs to the public.
•	Ethical use of data - what does it mean for data collection for DEQ? Do we need to assess our data collection methods with ""ethical use"" in mind? What yardstick do we use to see if our data collection & analysis is ethical?
•	What kind of implications does this have for how we handle data we gather from customers and communities? Need for transparency about how we share and what we will use it for. 
•	What constraints does this put on 3rd party use of our data? There are trade-offs between privacy and transparency. What if someone uses our data for questionable purposes, for example, adjusting insurance claims based on polluted areas? Often those who request our data have the resources and financial incentives to take advantage of it in ways disadvantaged communities cannot.
•	There are different types of open source models out there that have different implications in regard to how data is used. Some restrict for commercial use. What kind is the state recommending?
•	In the present moment, a focus on equity needs to be strictly defined. Where is our monitoring based (e.g. air quality measurements may be very different in affluent white versus poor neighborhoods of color)? We can’t have equity without starting with good data.
•	How do we communicate data in ways that is accessible to everyone? At present all our data is technically “public” but only people who know and have the time to work the issue will get it. How to make sure data is cleaned up and accessible to general public? 
•	How proactive ought we  be in terms of posting data publicly? A recent example being should we release DEQ’s data about use of tear gas preemptively or only on request?
•	Community engagement must become a critical part, which means committing resources to build capacity. Are there resources attached to this strategy for these efforts?
•	How will use of visualizations and communication tools be encouraged? In some projects we’ve gotten pushback (both internal and external) about making data too easily accessible.

Data lifecycles and retention:
 
•	There is a reference to ""data lifecycle"".  Does the state have guidance on the data lifecycle?
•	Theoretically retention schedules apply to data same as any other record, but in practice applying retention can look very different for structured data, especially when considering how data may be repurposed for future uses.
•	Data collected for one purpose may end up having value for other purposes later—e.g. today’s regulatory data may be tomorrow’s environmental data. Given that environmental trends can span centuries, the ""public good"" would mean keeping the data for centuries. But that has implications for our storage formats, the software we use to access and manipulate data, and for the storage of metadata and algorithms.
•	Databases get old- what happens when we need to migrate? Right now we need to maintain old software to access old data but that software is no longer supported—what happens when we can’t access? Who will pay to convert and clean up old data to make accessible for future?
•	State Archives has a solution for long term storage of electronic documents but not for data at this point. DEQ’s lab alone has terabytes of data.
•	We have only a marginal idea of what data management will look like down the road, and so maintaining flexibility is critical for changing data landscapes. A driving feature of flexibility should be efficiency. When reliable, secure, and cost-effective technology becomes available, we should seriously consider how we might adapt our practices. We should actively seek out ways to improve performance while keeping costs down.
•	Should the data strategy address maintaining data integrity - backups, ways to prevent hacking and other sources of data corruption? 
•	We should address how to handle duplicated data stored in different agencies and departments within an agency. The gold standard would be a master data base and everything else is a copy- if you want the definitive version you go to the source. But that would require a uniform process and audit trail to sync source and copies. How could this be implemented within/between agencies?
•	We work with numerous databases that are shared with other agencies/municipalities (e.g. Portland Harbor). The state should consider a directory of these with their designated owners. As much as possible we ought to link data together from different sources rather than duplicating.
•	What does the data lifecycle mean for new data we may gather? Ideally we should decide in advance how we plan to gather, analyze, communicate and retain the data and build that into the architecture. Are there good examples of this? DEQ may be able to provide some from our water quality integrated report."
"2020-08-24T16:25:19.000","Becky Chiao","Office the Public Records Advocate","becky.chiao@oregon.gov","The Oregon Data Strategy recognizes that data is an asset to be managed for the public good.  The articulated principles of effective governance and ethical use will foster a culture of learning, empowerment, and leadership for state agencies and the public.  

I support the recognition that currently there are cultural barriers to sharing data which must be overcome.  As the Public Records Advocate, I look forward to seeing the impact of the data strategy to increase transparency and broad public access to meaningful data.   The draft plan creates a strong foundation for future work."
"2020-08-24T16:44:37.000","Kathy Boles, Rich Marvin, Ben Scandella","Oregon Water Resources Department","Benjamin.P.Scandella@oregon.gov","The draft Oregon Data Strategy (last accessed 8/18/20) includes sound Data Principles and Identified Practices. We at the Oregon Water Resources Department (OWRD) would like to emphasize that:

A.	 As other commenters have noted, fully implementing these practices is a significant project. In the context of a contracting state budget, this work must be balanced with an agency’s other responsibilities. It would be helpful to have a vision for implementing these practices in a limited-bandwidth environment. For example, could we start with pilot projects that offer clear information about data most often requested by other agencies and the public?

B.	Principle 2 (“Leverage”) is an important goal, and the practices used to support it could use some expansion. Contextualizing data (Practice #6) requires more than simply creating data documentation of the data, because the entities being represented by some databases are so complex. Answering even apparently simple questions, like finding the volume of water permitted for irrigation use each year, requires detailed understanding of water rights, the database structure, and data quality. It is impossible for OWRD to anticipate every conceivable question that might be asked and explain the necessary assumptions or create a view table for it, but we continue to do so for the questions most commonly asked by our staff, other agencies, and the public. This context-specific nature of the water rights database also requires care when applying Practice #5, “Recognizing the value of data beyond the initial purpose for which it was collected…” because one could easily obtain an incorrect answer by attempting to relate components of the database that should not be related. OWRD offers a number of refined query and geospatial tools for water rights, surface water, groundwater, and well report data through our website, and we support linking to these from data.oregon.gov.

C.	Identifying appropriate standards for sharing data across agencies requires time-consuming coordination between agencies, and that coordination will be difficult given variable capacity to address Open Data. Choosing standards is further complicated by the tension between the desire for standards that are universal across disciplines and those that are shared broadly within a particular discipline. For example, WaterML is an internationally endorsed exchange standard specifically designed for hydrologic time series data that is not generally applicable to other types of data transfer. Similar challenges will also need to be addressed relating to Geospatial data."
